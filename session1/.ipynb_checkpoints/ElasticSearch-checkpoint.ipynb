{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1 - ElasticSearch - Zipf/Heaps laws"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Running Elastic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " During the first part of this session we will configure and run an ElasticSearch instance. \n",
    "\n",
    "**Read the first section of the documentation and follow its instructions.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After following the instructions you should test if ElasticSearch is up and running using the script `elastic-test.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer that you should get is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'{\\n  \"name\" : \"SJF0JlN\",\\n  \"cluster_name\" : \"elasticsearch_qiaoruixiang\",\\n  \"cluster_uuid\" : \"HvvvVQVCTOOmvhfRa3RKAw\",\\n  \"version\" : {\\n    \"number\" : \"6.2.4\",\\n    \"build_hash\" : \"ccec39f\",\\n    \"build_date\" : \"2018-04-12T20:37:28.497551Z\",\\n    \"build_snapshot\" : false,\\n    \"lucene_version\" : \"7.2.1\",\\n    \"minimum_wire_compatibility_version\" : \"5.6.0\",\\n    \"minimum_index_compatibility_version\" : \"5.0.0\"\\n  },\\n  \"tagline\" : \"You Know, for Search\"\\n}\\n'\n"
     ]
    }
   ],
   "source": [
    "%run elastic_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 2 Indexing and querying\n",
    "\n",
    "**Take a moment to read section 2.1 of the documentation **\n",
    "\n",
    "ElasticSearch is a database that allows storing documents (tables do not need a predefined schema as in relational databases). Text in these documents can be processed so the queries extend beyond exact matches allowing complex queries, fuzzy matching and ranking documents respect to the actual match. \n",
    "\n",
    "These kind of databases are behind search engines like Google Search or Bing.\n",
    "\n",
    "There are different ways of operating with ElasticSearch. It is deployed esentially as a web service with a REST API, so we can accessed basically from any language with a library for operating with HTTP servers. You have a link to the full documentation in the session document.\n",
    "\n",
    "We are going to use two python libraries for programming `elasticsearch` and `elasticsearch-dsl`. Both provide access to ElasticSearch functionalities hidding and making more programming friendly the interactions, the second one is more convenient for configurating and searching.\n",
    "\n",
    "We are only going to see the essential elements for developing the session but feel free to learn a little bit more. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will need some text to index, for testing purposes we are going to use the python library `loremipsum`. We will need to install it first (if it is not installed already)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting loremipsum\n",
      "\u001b[33m  Cache entry deserialization failed, entry ignored\u001b[0m\n",
      "  Using cached https://files.pythonhosted.org/packages/55/8e/f75963c116c72bb81d2e22ec64ff3837e962cc89bae025ab60698dd83160/loremipsum-1.0.5.tar.gz\n",
      "Building wheels for collected packages: loremipsum\n",
      "  Running setup.py bdist_wheel for loremipsum ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/qiaoruixiang/Library/Caches/pip/wheels/cd/4c/6c/33f9c3db3fcd070c9021f5b4c25d1b3ddd1596ccffbbc0d6ff\n",
      "Successfully built loremipsum\n",
      "Installing collected packages: loremipsum\n",
      "Successfully installed loremipsum-1.0.5\n",
      "\u001b[33mCache entry deserialization failed, entry ignored\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install loremipsum --user  # Restart the kernel to be able to import the library in the next cells"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To interact with ElasticSearch with need a client object of type `Elasticsearch`, if we have running the server in the localhost and with the default configuration we don't need to pass any parameters to the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "client = Elasticsearch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this client you have a connection for operatinh with Elastic search. Now we will create an index. There are index operations in each libraty, but the one in `elasticseach-dsl` is simpler to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch_dsl import Index\n",
    "\n",
    "index = Index('test', using=client)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create some random paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'loremipsum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-fdc38e169dfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mloremipsum\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mget_paragraphs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_paragraphs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'loremipsum'"
     ]
    }
   ],
   "source": [
    "from loremipsum import get_paragraphs\n",
    "text = get_paragraphs(10)\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can index the paragraphs in ElasticSearch using the `create` method, we can indicate a type of document that will allow to group documents of the same king inside an index. The document is passed as the `body` parameter as a python dictionary. The keys of the dictionary will be the fields of the document, in this case we well have only one (`text`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-50d51ce346dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'latin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "for t in text:\n",
    "    client.index(index='test', doc_type='latin', body={'text': t})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can search the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch_dsl import Search\n",
    "s = Search(using=client, index='test')\n",
    "\n",
    "s = s.query('match', text='Netus')\n",
    "\n",
    "r = s.execute()\n",
    "\n",
    "for v in r:\n",
    "    print('ID= %s Text= %s' % (v.meta.id, v.text[:75]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 2.1 Anatomy of an indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready for indexing some files, download the two sets of files linked in the documentation (*20_newsgroups* and *novels*) and follow the instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Follow the instructions** and after that edit the script `IndexFiles.py` and understand how the indexing is performed, you will see that instead of inserting the documents one by one the `bulk` method is used for a more efficient indexing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 2.2 Looking for mr goodword\n",
    "\n",
    "Now we are ready for query the documents. You have the script `SearchIndex.py` for this purpose, you can invoke the script with three flags:\n",
    "\n",
    "* `--index` that correponds with the index of the files\n",
    "* `--text` that searches for a word in the text field of the documents of the index\n",
    "* `--query` that allows using LUCENE syntax for querying the index\n",
    "\n",
    "\n",
    "These last two flags are mutually exclusive and the first one takes precedence\n",
    "\n",
    "LUCENE syntax allows to use boolean operators in the query (AND, OR, NOT) always upper case and the fuzzy operator `~` with a number $n$ that matches the word allowing up to $n$ mismatches in the string.\n",
    "\n",
    "**Follow the instructions** of the documentation and query the documents indexed. Browse the code and look into the documentation of `elasticsearch-dsl` to learn more about how a query is defined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## 3 Zipf's and Heaps' Laws\n",
    "\n",
    "Now we can work in the tasks for this session. You will have to test if the Zipf and Heaps Laws hold in the documents that you have.\n",
    "\n",
    "You will need a count of the words in all the documents. ElasticSearch allows querying these counts from the ids of the documents.\n",
    "\n",
    "For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from elasticsearch.helpers import scan\n",
    "\n",
    "# Search for all the documents and query the list of (word, frequency) of each one\n",
    "# Totals are accumulated in a dictionary\n",
    "voc = {}\n",
    "sc = scan(client, index='test', doc_type='latin', query={\"query\" : {\"match_all\": {}}})\n",
    "for s in sc:\n",
    "    tv = client.termvectors(index='test', doc_type='latin', id=s['_id'], fields=['text'])\n",
    "    if 'text' in tv['term_vectors']:\n",
    "        for t in tv['term_vectors']['text']['terms']:\n",
    "            if t in voc:\n",
    "                voc[t] += tv['term_vectors']['text']['terms'][t]['term_freq']\n",
    "            else:\n",
    "                voc[t] = tv['term_vectors']['text']['terms'][t]['term_freq']\n",
    "\n",
    "                lpal = sorted(voc.items(), reverse=True, key=lambda x: x[1])\n",
    "\n",
    "pal, freq = [p for p, _ in lpal], [f for _, f in lpal]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can plot the words frequencies (have in mind that this text is artifically generated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "fig = plt.figure(figsize=(20,10))\n",
    "plt.bar(range(len(pal)), freq)\n",
    "a= plt.xticks(range(len(pal)), pal, rotation='vertical')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `CountWords.py` script will generate the list of words and their frequency for an index. \n",
    "\n",
    "**Follow the instructions** in the documentation and **pay attention** to the documentation that you have to deliver for this session. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "135px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
